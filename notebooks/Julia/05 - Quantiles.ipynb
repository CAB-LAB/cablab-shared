{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ESDL\n",
    "using WeightedOnlineStats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook we want to visualize the regions where cube variable take on their highest or lowest value. To do this we calculate the 99% and the 1% quantile for each variable from a subset of the data and afterwards count how often the quantile was exceeded for each variable in each grid cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c             = Cube()\n",
    "vars          = [\"air_temperature_2m\",\"soil_moisture\",\"gross_primary_productivity\"];\n",
    "cdata         = subsetcube(c,region=\"South America\",variable=vars,time=2001:2002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it might seem to be a trivial task to estimate a quantile of some datacube variables, it actually isn't. For an exact quantile estimate, the whole dataset would need to be in memory. However, it is possible to get aproximate estimates of the quantiles by first fitting a `WeightedHistogram` provided by the [WeightedOnlineStats](https://github.com/gdkrmr/WeightedOnlineStats.jl) package and estimate the quantiles based on the Histogram. We first generate an iterable Table from the cube data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = CubeTable(cdata = cdata, include_axes=(\"lat\",\"var\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a row iterator over the cube which will load data on demand. To fit a `WeightedHist` split by variable and weighted by cosine-latitude weights we use the convenience function `cubefittable` provided by the ESDL framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = cubefittable(t,WeightedAdaptiveHist(200),:cdata,by=(:var,), weight=i->cosd(i.lat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ESDLPlots\n",
    "plotHist(o,var=\"soil_moisture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This datacube encodes a weighted Histogram for every variable with 100 bins. We can estimate the 1% and 99% quantiles from the Histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = quantile(o,[0.01,0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the estimated quantiles to count for each pixel how often the quantile has been exceeded. TO do this, we define a function that counts quantile crossings for each grid cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "How often the upper or lower quantiles are crossed in each time series\n",
    "\"\"\"\n",
    "function countExtremes(xout::AbstractArray,xin::AbstractVector,qvec::AbstractVector)\n",
    "    nlow,nhigh=0,0\n",
    "    qlow,qhigh=qvec\n",
    "    for v in xin\n",
    "        if !ismissing(v)\n",
    "            v<=qlow && (nlow+=1)\n",
    "            v>=qhigh && (nhigh+=1)\n",
    "        end\n",
    "    end\n",
    "    xout[1]=nhigh\n",
    "    xout[2]=nlow\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indims = InDims(\"Time\"),InDims(\"Quantile\")\n",
    "outdims = OutDims(CategoricalAxis(\"Direction\",[\"High\",\"Low\"]),outtype=Int)\n",
    "tresexcount=mapCube(countExtremes,(cdata,q),\n",
    "  indims = indims,\n",
    "  outdims = outdims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the low extremes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for v in getAxis(\"Var\",o).values\n",
    "display(v)\n",
    "display(plotMAP(tresexcount,dmax=10,dir=\"High\",var=v))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "And the high extremes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for v in getAxis(\"Var\",o).values\n",
    "display(v)\n",
    "display(plotMAP(tresexcount,dmax=10,dir=\"Low\",var=v))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
